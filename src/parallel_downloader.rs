use anyhow::Result;
use futures::{stream, StreamExt};
use std::time::{Duration, Instant};
use tracing::{info, warn};

use crate::{Document, EnhancedRAGArticleGenerator};

/// –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
const MAX_CONCURRENT_DOWNLOADS: usize = 8;

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
#[derive(Debug)]
pub struct DownloadStats {
    pub total_urls: usize,
    pub successful: usize,
    pub failed: usize,
    pub total_bytes: usize,
    pub elapsed_time: Duration,
    pub throughput: f64, // –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
}

impl EnhancedRAGArticleGenerator {
    /// –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º concurrency
    /// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç futures::stream::buffer_unordered –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    pub async fn load_and_process_documents_parallel(
        &self,
        urls: Vec<String>,
    ) -> Result<Vec<Document>> {
        let (documents, stats) = self
            .load_documents_with_stats(urls, MAX_CONCURRENT_DOWNLOADS)
            .await?;

        info!("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏:");
        info!("  ‚úÖ –£—Å–ø–µ—à–Ω–æ: {} –∏–∑ {}", stats.successful, stats.total_urls);
        info!("  ‚ùå –û—à–∏–±–æ–∫: {}", stats.failed);
        info!("  ‚è±Ô∏è –í—Ä–µ–º—è: {:.2}—Å", stats.elapsed_time.as_secs_f32());
        info!("  üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {:.1} –¥–æ–∫/—Å–µ–∫", stats.throughput);
        info!(
            "  üíæ –î–∞–Ω–Ω—ã—Ö: {:.2} –ú–ë",
            stats.total_bytes as f64 / 1_048_576.0
        );

        Ok(documents)
    }

    /// –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –ª–∏–º–∏—Ç–æ–º concurrency
    pub async fn load_documents_with_concurrency_limit(
        &self,
        urls: Vec<String>,
        concurrent_limit: usize,
    ) -> Result<Vec<Document>> {
        let (documents, _) = self
            .load_documents_with_stats(urls, concurrent_limit)
            .await?;
        Ok(documents)
    }

    /// –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    pub async fn load_documents_with_stats(
        &self,
        urls: Vec<String>,
        concurrent_limit: usize,
    ) -> Result<(Vec<Document>, DownloadStats)> {
        if urls.is_empty() {
            return Ok((Vec::new(), DownloadStats::default()));
        }

        info!(
            "üöÄ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ {} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (concurrency: {})",
            urls.len(),
            concurrent_limit
        );

        let start_time = Instant::now();

        // –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(30))
            .tcp_keepalive(Duration::from_secs(10))
            .pool_max_idle_per_host(10)
            .user_agent("Enhanced-RAG-Generator/2.0")
            .build()?;

        // –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        let mut successful = 0;
        let mut failed = 0;
        let mut total_bytes = 0;

        // –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫ futures –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        let download_stream = stream::iter(urls.iter().enumerate())
            .map(|(index, url)| {
                let client = client.clone();
                let url = url.clone();

                async move {
                    info!(
                        "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {} –æ—Ç {}",
                        index + 1,
                        Self::truncate_url(&url, 50)
                    );

                    match self.download_and_process_document(&client, &url).await {
                        Ok(doc) => {
                            // info!(
                            //     "‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {} –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ ({} —Å–∏–º–≤–æ–ª–æ–≤)",
                            //     index + 1,
                            //     doc.clone().page_content.len()
                            // );
                            Ok((doc.clone(), doc.page_content.len()))
                        }
                        Err(e) => {
                            warn!(
                                "‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {} ({}): {}",
                                index + 1,
                                Self::truncate_url(&url, 30),
                                e
                            );
                            Err(e)
                        }
                    }
                }
            })
            // ‚≠ê –ö–õ–Æ–ß–ï–í–ê–Ø –°–¢–†–û–ö–ê: buffer_unordered –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç concurrency
            .buffer_unordered(concurrent_limit);

        // –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö
        let mut documents = Vec::new();
        let mut results = download_stream.collect::<Vec<_>>().await;

        for result in results.drain(..) {
            match result {
                Ok((doc, bytes)) => {
                    successful += 1;
                    total_bytes += bytes;
                    documents.push(doc);
                }
                Err(_) => {
                    failed += 1;
                    // –û—à–∏–±–∫–∞ —É–∂–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞ –≤—ã—à–µ
                }
            }
        }

        let elapsed_time = start_time.elapsed();
        let throughput = successful as f64 / elapsed_time.as_secs_f64();

        let stats = DownloadStats {
            total_urls: urls.len(),
            successful,
            failed,
            total_bytes,
            elapsed_time,
            throughput,
        };

        info!(
            "üéâ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {:.2}—Å",
            elapsed_time.as_secs_f32()
        );

        Ok((documents, stats))
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç
    async fn download_and_process_document(
        &self,
        client: &reqwest::Client,
        url: &str,
    ) -> Result<Document> {
        // HTTP –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        let response = client
            .get(url)
            .send()
            .await
            .map_err(|e| anyhow::anyhow!("–û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {}", e))?;

        if !response.status().is_success() {
            return Err(anyhow::anyhow!(
                "HTTP –æ—à–∏–±–∫–∞ {}: {}",
                response.status(),
                url
            ));
        }

        let content = response
            .text()
            .await
            .map_err(|e| anyhow::anyhow!("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {}", e))?;

        // –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if content.len() < 100 {
            return Err(anyhow::anyhow!(
                "–ö–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {} —Å–∏–º–≤–æ–ª–æ–≤ (–º–∏–Ω–∏–º—É–º 100)",
                content.len()
            ));
        }

        if content.len() > 1_000_000 {
            // 1MB –ª–∏–º–∏—Ç
            return Err(anyhow::anyhow!(
                "–ö–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {} —Å–∏–º–≤–æ–ª–æ–≤ (–º–∞–∫—Å–∏–º—É–º 1M)",
                content.len()
            ));
        }

        // –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        let mut metadata = std::collections::HashMap::new();
        metadata.insert("source_url".to_string(), url.to_string());
        metadata.insert("content_length".to_string(), content.len().to_string());
        metadata.insert("download_time".to_string(), chrono::Utc::now().to_rfc3339());
        metadata.insert(
            "content_hash".to_string(),
            format!("{:x}", md5::compute(&content)),
        );

        // –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if let Ok(parsed_url) = url::Url::parse(url) {
            if let Some(host) = parsed_url.host_str() {
                metadata.insert("domain".to_string(), host.to_string());
            }
        }

        Ok(Document {
            page_content: content,
            metadata,
        })
    }

    /// –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
    pub async fn download_with_retry(
        &self,
        client: &reqwest::Client,
        url: &str,
        max_retries: u32,
    ) -> Result<Document> {
        let mut attempts = 0;
        let mut last_error = None;

        while attempts <= max_retries {
            match self.download_and_process_document(client, url).await {
                Ok(doc) => return Ok(doc),
                Err(e) => {
                    last_error = Some(e);
                    attempts += 1;

                    if attempts <= max_retries {
                        let delay = Duration::from_secs(2u64.pow(attempts.min(5))); // Cap at 32s
                        warn!(
                            "‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {} –Ω–µ—É–¥–∞—á–Ω–∞ –¥–ª—è {}, –ø–æ–≤—Ç–æ—Ä—è–µ–º —á–µ—Ä–µ–∑ {:?}",
                            attempts,
                            Self::truncate_url(url, 40),
                            delay
                        );
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }

        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ retry")))
    }

    /// –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö URL –≤ –ª–æ–≥–∞—Ö
    fn truncate_url(url: &str, max_len: usize) -> String {
        if url.len() <= max_len {
            url.to_string()
        } else {
            format!("{}...", &url[..max_len.saturating_sub(3)])
        }
    }
}

impl Default for DownloadStats {
    fn default() -> Self {
        Self {
            total_urls: 0,
            successful: 0,
            failed: 0,
            total_bytes: 0,
            elapsed_time: Duration::from_secs(0),
            throughput: 0.0,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_truncate_url() {
        let long_url = "https://example.com/very/long/path/that/exceeds/limit";
        let truncated = EnhancedRAGArticleGenerator::truncate_url(long_url, 20);
        assert!(truncated.len() <= 20);
        assert!(truncated.ends_with("..."));
    }

    #[tokio::test]
    async fn test_parallel_download_empty_urls() {
        // –¢–µ—Å—Ç —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º URL
        let generator = EnhancedRAGArticleGenerator::new(
            "http://test".to_string(),
            "test-model".to_string(),
            "test-embed".to_string(),
            None,
        );

        let result = generator.load_documents_with_stats(vec![], 5).await;
        assert!(result.is_ok());

        let (docs, stats) = result.unwrap();
        assert_eq!(docs.len(), 0);
        assert_eq!(stats.total_urls, 0);
        assert_eq!(stats.successful, 0);
        assert_eq!(stats.failed, 0);
    }
}
